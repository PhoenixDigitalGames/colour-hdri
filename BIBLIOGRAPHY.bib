@incollection{AdobeSystems2012d,
  title        = {Translating White Balance xy Coordinates to Camera
    Neutral Coordinates},
  booktitle    = {Digital Negative (DNG) Specification},
  author       = {{Adobe Systems}},
  year         = 2012,
  pages        = 80,
  abstract     = {This publication and the information herein is
    furnished AS IS, is subject to change without notice, and should
    not be construed as a commitment by Adobe Systems Incorporated.
    Adobe Systems Incorporated assumes no responsibility or liability
    for any errors or inaccuracies, makes no warranty of any kind
    (express, implied, or statutory) with respect to this publication,
    and expressly disclaims any and all warranties of merchantability,
    fitness for particular purposes, and noninfringement of third
    party rights.},
}
@incollection{AdobeSystems2012e,
  title        = {Translating Camera Neutral Coordinates to White
    Balance xy Coordinates},
  booktitle    = {Digital Negative (DNG) Specification},
  author       = {{Adobe Systems}},
  year         = 2012,
  pages        = {80--81},
  abstract     = {This publication and the information herein is
    furnished AS IS, is subject to change without notice, and should
    not be construed as a commitment by Adobe Systems Incorporated.
    Adobe Systems Incorporated assumes no responsibility or liability
    for any errors or inaccuracies, makes no warranty of any kind
    (express, implied, or statutory) with respect to this publication,
    and expressly disclaims any and all warranties of merchantability,
    fitness for particular purposes, and noninfringement of third
    party rights.},
}
@misc{AdobeSystems2012f,
  title        = {Digital Negative (DNG) Specification},
  author       = {{Adobe Systems}},
  year         = 2012,
  pages        = {1--101},
  abstract     = {This publication and the information herein is
    furnished AS IS, is subject to change without notice, and should
    not be construed as a commitment by Adobe Systems Incorporated.
    Adobe Systems Incorporated assumes no responsibility or liability
    for any errors or inaccuracies, makes no warranty of any kind
    (express, implied, or statutory) with respect to this publication,
    and expressly disclaims any and all warranties of merchantability,
    fitness for particular purposes, and noninfringement of third
    party rights.},
}
@incollection{AdobeSystems2012g,
  title        = {Camera to XYZ (D50) Transform},
  booktitle    = {Digital Negative (DNG) Specification},
  author       = {{Adobe Systems}},
  year         = 2012,
  pages        = 81,
  abstract     = {This publication and the information herein is
    furnished AS IS, is subject to change without notice, and should
    not be construed as a commitment by Adobe Systems Incorporated.
    Adobe Systems Incorporated assumes no responsibility or liability
    for any errors or inaccuracies, makes no warranty of any kind
    (express, implied, or statutory) with respect to this publication,
    and expressly disclaims any and all warranties of merchantability,
    fitness for particular purposes, and noninfringement of third
    party rights.},
}
@misc{AdobeSystems2015c,
  title        = {Adobe DNG SDK 1.4 -
    dng\_sdk\_1\_4/dng\_sdk/source/dng\_camera\_profile.cpp -
    dng\_camera\_profile::IlluminantToTemperature},
  author       = {{Adobe Systems}},
  year         = 2015,
  url          = {http://download.adobe.com/pub/adobe/dng/dng_sdk_1_4.zip},
}
@misc{AdobeSystems2015d,
  title        = {Adobe DNG SDK 1.4},
  author       = {{Adobe Systems}},
  year         = 2015,
  url          = {http://download.adobe.com/pub/adobe/dng/dng_sdk_1_4.zip},
}
@misc{AdobeSystems2015e,
  title        = {Adobe DNG SDK 1.4 -
    dng\_sdk\_1\_4/dng\_sdk/source/dng\_tag\_values.h - LightSource
    tag},
  author       = {{Adobe Systems}},
  year         = 2015,
  url          = {http://download.adobe.com/pub/adobe/dng/dng_sdk_1_4.zip},
}
@incollection{Banterle2011k,
  title        = {3.2.1 Simple Mapping Methods},
  booktitle    = {Advanced High Dynamic Range Imaging},
  author       = {Banterle, Francesco and Artusi, Alessandro and
    Debattista, Kurt and Chalmers, Alan},
  year         = 2011,
  pages        = {38--41},
  publisher    = {A K Peters/CRC Press},
  isbn         = {978-1-56881-719-4},
}
@book{Banterle2011n,
  title        = {2.1.1 Generating HDR Content by Combining Multiple
    Exposures},
  author       = {Banterle, Francesco and Artusi, Alessandro and
    Debattista, Kurt and Chalmers, Alan},
  year         = 2011,
  publisher    = {A K Peters/CRC Press},
  isbn         = {978-1-56881-719-4},
  journal      = {Advanced High Dynamic Range Imaging},
}
@misc{Banterle2014a,
  title        = {PICCANTE: An Open and Portable Library for HDR
    Imaging},
  author       = {Banterle, Francesco and Benedetti, Luca},
  year         = 2014,
}
@misc{Coffin2015a,
  title        = {dcraw},
  author       = {Coffin, Dave},
  year         = 2015,
  url          = {https://www.cybercom.net/~dcoffin/dcraw/},
}
@inproceedings{Debevec1997a,
  title        = {Recovering high dynamic range radiance maps from
    photographs},
  booktitle    = {Proceedings of the 24th annual conference on
    Computer graphics and interactive techniques - SIGGRAPH '97},
  author       = {Debevec, Paul E. and Malik, Jitendra},
  year         = 1997,
  pages        = {369--378},
  publisher    = {ACM Press},
  address      = {New York, New York, USA},
  issn         = 00978930,
  doi          = {10.1145/258734.258884},
  abstract     = {We present a method of recovering high dynamic range
    radiance maps from photographs taken with conventional imaging
    equip- ment. In our method, multiple photographs of the scene are
    taken with different amounts of exposure. Our algorithm uses these
    dif- ferently exposed photographs to recover the response function
    of the imaging process, up to factor of scale, using the
    assumption of reci- procity. With the known response function, the
    algorithm can fuse themultiple photographs into a single, high
    dynamic range radiance map whose pixel values are proportional to
    the true radiance values in the scene. We demonstrate our method
    on images acquired with both photochemical and digital imaging
    processes. We discuss how this work is applicable in many areas of
    computer graphics involv- ing digitized photographs, including
    image-based modeling, image compositing, and image processing.
    Lastly, we demonstrate a few applications of having high dynamic
    range radiance maps, such as synthesizing realistic motion blur
    and simulating the response of the human visual system.},
  isbn         = {0-89791-896-7},
}
@article{Grossberg2003g,
  title        = {Determining the camera response from images: What is
    knowable?},
  author       = {Grossberg, M.D. and Nayar, S.K.},
  year         = 2003,
  month        = nov,
  volume       = 25,
  pages        = {1455--1467},
  issn         = {0162-8828},
  doi          = {10.1109/TPAMI.2003.1240119},
  abstract     = {An image acquired by a camera consists of measured
    intensity values which are related to scene radiance by a function
    called the camera response function. Knowledge of this response is
    necessary for computer vision algorithms which depend on scene
    radiance. One way the response has been determined is by
    establishing a mapping of intensity values between images taken
    with different exposures. We call this mapping the intensity
    mapping function. In this paper, we address two basic questions.
    What information from a pair of images taken at different
    exposures is needed to determine the intensity mapping function?
    Given this function, can the response of the camera and the
    exposures of the images be determined? We completely determine the
    ambiguities associated with the recovery of the response and the
    ratios of the exposures. We show all methods that have been used
    to recover the response break these ambiguities by making
    assumptions on the exposures or on the form of the response. We
    also show when the ratio of exposures can be recovered directly
    from the intensity mapping, without recovering the response. We
    show that the intensity mapping between images is determined
    solely by the intensity histograms of the images. We describe how
    this allows determination of the intensity mapping between images
    without registration. This makes it possible to determine the
    intensity mapping in sequences with some motion of both the camera
    and objects in the scene.},
  isbn         = {0-7695-1900-8},
  journal      = {IEEE Transactions on Pattern Analysis and Machine
    Intelligence},
  keywords     = {Ambiguities,Calibration,Comparagram,Comparametric,Dynamic
    range,Histogram,Histogram specification,Illumination,Intensity
    mapping,Radiometry,Response function},
  number       = 11,
}
@misc{Habble2010d,
  title        = {Filmic Tonemapping Operators},
  author       = {Habble, John},
  year         = 2010,
  url          = {http://filmicgames.com/archives/75},
  urldate      = {2015-03-15},
}
@misc{Habble2010e,
  title        = {Uncharted 2: HDR Lighting},
  author       = {Habble, John},
  year         = 2010,
  url          = {http://www.slideshare.net/ozlael/hable-john-uncharted2-hdr-lighting},
  urldate      = {2015-03-15},
}
@misc{ISO2006,
  title        = {INTERNATIONAL STANDARD ISO12232-2006 - Photography -
    Digital still cameras - Determination of exposure index, ISO speed
    ratings, standard output sensitivity, and recommended exposure
    index},
  author       = {{ISO}},
  year         = 2006,
}
@article{Lagarde2014,
  title        = {Moving Frostbite to Physically Based Rendering 3.0},
  author       = {Lagarde, S{\'e}bastian and {de Rousiers}, Charles},
  year         = 2014,
  pages        = 119,
  journal      = {Siggraph 2014},
  keywords     = {frostbite,pbr,physically based rendering},
}
@misc{Lagarde2016b,
  title        = {An Artist-Friendly Workflow for Panoramic HDRI},
  author       = {Lagarde, Sebastien and Lachambre, Sebastien and
    Jover, Cyril},
  year         = 2016,
  url          = {http://blog.selfshadow.com/publications/s2016-shading-course/unity/s2016_pbs_unity_hdri_notes.pdf},
}
@misc{McGuffog2012a,
  title        = {Hue Twists in DNG Camera Profiles},
  author       = {McGuffog, Sandy},
  year         = 2012,
  url          = {http://dcptool.sourceforge.net/Hue\%20Twists.html},
  urldate      = {2016-10-29},
}
@article{Reinhard2005c,
  title        = {Dynamic Range Reduction Inspired by Photoreceptor
    Physiology},
  author       = {Reinhard, Erik and Devlin, Kate},
  year         = 2005,
  month        = jan,
  volume       = 11,
  pages        = {13--24},
  issn         = {1077-2626},
  doi          = {10.1109/TVCG.2005.9},
  abstract     = {A common task in computer graphics is the mapping of
    digital high dynamic range images to low dynamic range display
    devices such as monitors and printers. This task is similar to the
    adaptation processes which occur in the human visual system.
    Physiological evidence suggests that adaptation already occurs in
    the photoreceptors, leading to a straightforward model that can be
    easily adapted for tone reproduction. The result is a fast and
    practical algorithm for general use with intuitive user parameters
    that control intensity, contrast, and level of chromatic
    adaptation, respectively.},
  isbn         = {1077-2626 (Print)\textbackslash{}r1077-2626
    (Linking)},
  journal      = {IEEE Transactions on Visualization and Computer
    Graphics},
  keywords     = {Dynamic range reduction,Photoreceptor
    physiology,Tone reproduction},
  number       = 01,
  pmid         = 15631125,
}
@article{Schlick1994,
  title        = {Quantization Techniques for Visualization of High
    Dynamic Range Pictures},
  author       = {Schlick, Christophe},
  year         = 1994,
  pages        = {7--18},
  issn         = {0920-5691},
  abstract     = {This paper proposes several techniques that enable
    to display high dynamic range pictures (created by a global
    illumination rendering program, for instance) on a low dynamic
    range device. The methods described here are based on some basic
    knowledge about human vision and are intended to provide
    "realistic looking" images on the visualization device, even with
    critical lighting conditions in the rendered scene. The main
    features of the new techniques are speed (only a handful of
    floating point operations per pixel are needed) and simplicity
    (only one single parameter, which can be empirically evaluated has
    to be provided by the user). The goal of this paper is not to
    propose a psychovisual or neurological model for subjective
    perception, but only to described some experimental results and
    propose some possible research directions.},
  journal      = {Proceedings of the Fifth Eurographics Workshop on
    Rendering},
  keywords     = {dynamic range,subjective brightness perception,tone
    reproduction},
  number       = {Section 5},
}
@article{Tumblin1999c,
  title        = {Two methods for display of high contrast images},
  author       = {Tumblin, Jack and Hodgins, Jessica K. and Guenter,
    Brian K.},
  year         = 1999,
  month        = jan,
  volume       = 18,
  pages        = {56--94},
  issn         = 07300301,
  doi          = {10.1145/300776.300783},
  abstract     = {High contrast images are common in night scenes and
    other scenes that include dark shadows and bright light sources.
    These scenes are difficult to display because their contrasts
    greatly exceed the range of most display devices for images. As a
    result, the image constrasts are compressed or truncated,
    obscuring subtle textures and details. Humans view and understand
    high contrast scenes easily, ``adapting'' their visual response to
    avoid compression or truncation with no apparent loss of detail.
    By imitating some of these visual adaptation processes, we
    developed methods for the improved display of high-contrast
    images. The first builds a display image from several layers of
    lighting and surface properties. Only the lighting layers are
    compressed, drastically reducing contrast while preserving much of
    the image detail. This method is practical only for synthetic
    images where the layers can be retained from the rendering
    process. The second method interactively adjusts the displayed
    image to preserve local contrasts in a small ``foveal''
    neighborhood. Unlike the first method, this technique is usable on
    any image and includes a new tone reproduction operator. Both
    methods use a sigmoid function for contrast compression. This
    function has no effect when applied to small signals but
    compresses large signals to fit within an asymptotic limit. We
    demonstrate the effectiveness of these approaches by comparing
    processed and unprocessed images.},
  isbn         = {0730-0301},
  journal      = {ACM Transactions on Graphics},
  number       = 1,
}
@inproceedings{Viriyothai2009,
  title        = {Variance minimization light probe sampling},
  booktitle    = {SIGGRAPH '09: Posters on - SIGGRAPH '09},
  author       = {Viriyothai, Kuntee and Debevec, Paul},
  year         = 2009,
  pages        = {1--1},
  publisher    = {ACM Press},
  address      = {New York, New York, USA},
  doi          = {10.1145/1599301.1599393},
  isbn         = {978-1-60558-726-4},
}
@misc{Wikipediabj,
  title        = {EV as a measure of luminance and illuminance},
  author       = {{Wikipedia}},
  url          = {https://en.wikipedia.org/wiki/Exposure_value\#EV_as_a_measure_of_luminance_and_illuminance},
  urldate      = {2015-11-14},
}
@misc{Wikipediabn,
  title        = {Tonemapping - Purpose and methods},
  author       = {{Wikipedia}},
  url          = {http://en.wikipedia.org/wiki/Tone_mapping\#Purpose_and_methods},
  urldate      = {2015-03-15},
}
